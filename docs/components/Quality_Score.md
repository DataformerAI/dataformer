# QualityScorer Component Documentation

## Overview
The `QualityScorer` class is designed to evaluate the quality of responses generated by a language model (LLM). It utilizes a Jinja2 template to format prompts and parse scores from the model's output. This component is useful for applications that require assessment of generated content based on specific criteria.

## Initialization
### `__init__(self, llm: AsyncLLM)`
- **Parameters**:
  - `llm`: An instance of `AsyncLLM`, which is the language model used for generating responses.
- **Description**: Initializes the `QualityScorer` with the specified language model and loads the scoring template.

## Methods

### `_load_template(self) -> Template`
- **Returns**: A Jinja2 `Template` object.
- **Description**: Loads the Jinja2 template from the specified file path. This template is used to format the prompts sent to the language model.

### `_parse_scores(self, output: Union[str, None], input: Dict[str, Any]) -> List[float]`
- **Parameters**:
  - `output`: The output string from the language model, which may contain score information.
  - `input`: A dictionary containing input data, including responses to be scored.
- **Returns**: A list of float scores corresponding to the input responses.
- **Description**: Parses the output from the language model to extract scores using a regular expression. If no output is provided, it returns a list of `None` values.

### `score(self, inputs: List[Dict[str, Any]], use_cache: bool = True) -> List[Dict[str, Any]]`
- **Parameters**:
  - `inputs`: A list of dictionaries, each containing an instruction and responses to be scored.
  - `use_cache`: A boolean flag indicating whether to use cached responses (default is `True`).
- **Returns**: A list of dictionaries containing the original inputs along with their corresponding scores and raw output.
- **Description**: Generates prompts using the loaded template, sends requests to the language model, and collects scores for each response. It also manages caching and task ID generation.

## Usage Example

### Example Input
```python
from dataformer.llms import AsyncLLM
from dataformer.components.quality_scorer import QualityScorer
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Initialize the language model
llm = AsyncLLM(
    model="meta-llama/Meta-Llama-3.1-8B-Instruct", api_provider="deepinfra"
)

# Create an instance of the QualityScorer class
quality_scorer = QualityScorer(llm=llm)

# Define inputs for scoring
inputs = [
    {
        "instruction": "Rate the following responses:",
        "responses": [
            "The capital of France is Paris.",
            "Photosynthesis is the process by which plants convert sunlight into energy."
        ]
    }
]

# Score the responses
results = quality_scorer.score(inputs)

# Print the results
for result in results:
    print(f"Instruction: {result['instruction']}")
    print(f"Responses: {result['responses']}")
    print(f"Scores: {result['scores']}")
    print(f"Raw Output: {result['raw output']}\n")
```

### Example Output
```plaintext
Instruction: Rate the following responses:
Responses: ['The capital of France is Paris.', 'Photosynthesis is the process by which plants convert sunlight into energy.']
Scores: [10.0, 9.5]
Raw Output: [1] Score: 10
[2] Score: 9.5
```

